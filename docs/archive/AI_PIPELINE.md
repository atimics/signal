# AI Pipeline: A Native, High-Performance Approach for a Persistent Universe

## 1. Executive Summary

This document outlines a proposed AI pipeline for CGame, designed to power a persistent universe with thousands of entities at scale. The core of this proposal is a transition from the general-purpose `llama.cpp` library to a more specialized, high-performance native C/C++ stack centered around `gemma.cpp` and the `ggml` tensor library. This will enable us to run Google's lightweight Gemma models with maximum efficiency, providing the foundation for a rich and dynamic AI-driven world.

The proposed pipeline is designed to be highly scalable, with a focus on low-latency inference, efficient task scheduling, and a modular architecture that can be easily extended and maintained.

## 2. Core Philosophy

The design of this AI pipeline is guided by the following principles:

*   **Native Performance:** The entire stack should be C/C++ native to ensure maximum performance and minimal overhead.
*   **Edge-Optimized:** The pipeline should be optimized for running small, fast language models like Gemma on the CPU.
*   **Scalability:** The architecture must be able to handle thousands of AI agents concurrently, with a robust system for managing tasks and resources.
*   **Modularity:** The AI system should be decoupled from the core game logic, allowing for independent development and testing.
*   **Data-Driven:** AI behaviors and personalities should be defined in data files, allowing for easy iteration and content creation.

## 3. Proposed Native Stack

The following C/C++ native libraries and tools are recommended for the AI pipeline:

*   **Inference Engine: `gemma.cpp`**
    *   **Why:** `gemma.cpp` is a lightweight, standalone C++ inference engine specifically designed for Gemma models. It is inspired by `ggml` and is highly optimized for CPU inference. Its minimal dependencies and focus on performance make it the ideal choice for our native stack.
*   **Tensor Library: `ggml`**
    *   **Why:** `ggml` is the underlying tensor library for `gemma.cpp`. It provides a powerful and efficient way to perform the mathematical operations required for neural network inference. Its C-native implementation and focus on performance make it a perfect fit for our stack.
*   **Task Scheduler: Custom C Implementation**
    *   **Why:** A custom C-based task scheduler will provide the flexibility and performance needed to manage thousands of AI tasks. The existing priority-based system is a good starting point, but it should be extended to support more advanced features like task dependencies and dynamic priority adjustments.
*   **Behavior Trees: Custom C Implementation**
    *   **Why:** A simple, data-driven behavior tree implementation in C will provide a flexible and intuitive way to define complex AI behaviors. Behavior trees are a well-established technique in game AI and are a good fit for our data-driven approach.

## 4. AI Pipeline Architecture

The proposed AI pipeline consists of the following stages:

1.  **Task Generation:** AI tasks are generated by various game systems (e.g., the dialog system, the combat system, the world simulation). These tasks are then added to a central AI task queue.
2.  **Task Scheduling:** The AI task scheduler runs every frame and selects the highest-priority tasks from the queue. The priority of a task is determined by its type (e.g., dialog > combat > navigation) and its LOD (Level of Detail), which is based on the entity's distance from the player.
3.  **Prompt Generation:** For each selected task, a context-aware prompt is generated. This prompt includes information about the entity's current state, its personality (from its base prompt), and the context of the task.
4.  **Inference:** The generated prompt is passed to the `gemma.cpp` inference engine, which runs the Gemma model and returns a response.
5.  **Behavior Execution:** The response from the inference engine is then used to drive the entity's behavior. This is where the behavior tree comes in. The response is used to select a branch of the behavior tree to execute, which in turn updates the entity's state and actions.

## 5. Implementation Plan

The integration of the new AI pipeline will be carried out in the following phases:

### Phase 1: Integration of `gemma.cpp`

*   **Goal:** Replace the existing `llama.cpp` integration with `gemma.cpp`.
*   **Tasks:**
    *   [ ] Integrate the `gemma.cpp` library into the project.
    *   [ ] Update the `Makefile` to compile and link `gemma.cpp`.
    *   [ ] Replace the `llama_generate` function with a new `gemma_generate` function that uses the `gemma.cpp` API.
    *   [ ] Test the new integration with a simple dialog task.

### Phase 2: Custom Task Scheduler

*   **Goal:** Implement a custom C-based task scheduler.
*   **Tasks:**
    *   [ ] Design and implement a priority-based task queue.
    *   [ ] Implement the task scheduler, which will select tasks from the queue based on priority and LOD.
    *   [ ] Integrate the new task scheduler with the existing AI systems.

### Phase 3: Behavior Trees

*   **Goal:** Implement a data-driven behavior tree system.
*   **Tasks:**
    *   [ ] Design and implement a simple behavior tree data structure.
    *   [ ] Implement a behavior tree interpreter that can execute behavior trees defined in data files.
    *   [ ] Create a set of basic behavior tree nodes (e.g., sequence, selector, action).
    *   [ ] Integrate the behavior tree system with the AI pipeline.

### Phase 4: Full Integration and Testing

*   **Goal:** Fully integrate the new AI pipeline and test it at scale.
*   **Tasks:**
    *   [ ] Update all AI systems to use the new pipeline.
    *   [ ] Create a test scene with thousands of AI entities to test the performance and scalability of the system.
    *   [ ] Profile and optimize the AI pipeline to ensure it meets the performance targets.

This phased approach will allow us to incrementally integrate the new AI pipeline while minimizing disruption to the existing codebase. The end result will be a powerful, flexible, and high-performance AI system that is capable of powering a rich and dynamic persistent universe.
